

# CAPINTEL
**Context-Aware Explainability for ML-Based Loan Decisions**

---

## Repository Structure

capintel/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ main.py # FastAPI application entrypoint
â”‚ â”‚ â”œâ”€â”€ config.py # OpenRouter model & generation config
â”‚ â”‚ â”œâ”€â”€ schemas.py # API request/response schemas
â”‚ â”‚ â”œâ”€â”€ prompts.py # ðŸ”’ Frozen system + role prompts
â”‚ â”‚ â”œâ”€â”€ llm_client.py # OpenRouter API client
â”‚ â”‚ â””â”€â”€ explain_service.py # Core explanation logic
â”‚ â”‚
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â””â”€â”€ README.md
â”‚
â”œâ”€â”€ contracts/
â”‚ â””â”€â”€ explanation_contract.json # ðŸ”’ Frozen ML â†’ GenAI interface
â”‚
â”œâ”€â”€ demo/
â”‚ â”œâ”€â”€ sample_inputs/ # Fixed demo payloads
â”‚ â”‚ â”œâ”€â”€ rejected_high_risk.json
â”‚ â”‚ â”œâ”€â”€ approved_borderline.json
â”‚ â”‚ â””â”€â”€ approved_low_risk.json
â”‚ â”‚
â”‚ â””â”€â”€ screenshots/
â”‚
â”œâ”€â”€ docs/
â”‚ â”œâ”€â”€ architecture.md
â”‚ â”œâ”€â”€ prompt_design.md
â”‚ â””â”€â”€ demo_flow.md
â”‚
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE


---

## Overview

CAPINTEL is a **GenAI-based explanation layer** designed to sit on top of
traditional machine learning loan decision systems.

The system follows a strict separation of responsibilities:
- **ML models** make approval/rejection decisions
- **SHAP** identifies key contributing factors
- **GenAI** explains those factors to different stakeholders

GenAI is **never used for decision-making**.

---

## Motivation

Loan decisions often lack:
- Clear customer communication
- Actionable support explanations
- Audit-ready compliance documentation

While explainability tools like SHAP exist, they are technical and not
audience-specific. CAPINTEL converts model outputs into **role-aware,
controlled explanations** without compromising compliance.

---

## System Architecture



[ ML Model (XGBoost) ]
â†“
[ SHAP Feature Attributions ]
â†“
[ Frozen Explanation Contract ]
â†“
[ GenAI Explanation Engine ]
â†“
Role-Based Natural Language Output


---

## Frozen Explanation Contract

The interface between ML/SHAP and GenAI is **explicitly frozen** to ensure
stability, safety, and auditability.

```json
{
  "decision": "Approved | Rejected",
  "risk_score": 0,
  "top_negative": ["High DTI", "High Utilization"],
  "top_positive": ["Stable Income"]
}

Contract Rules

GenAI consumes this structure exactly as provided

No additional features are inferred or invented

GenAI cannot override or reinterpret the decision

ML and SHAP implementations may evolve independently

Role-Based Explanations

CAPINTEL generates explanations for three distinct roles:

Customer

Friendly, non-technical language

No risk scores or model references

Includes improvement suggestions

Support Agent

Structured and concise

Includes risk score and key drivers

Actionable talking points

Compliance Auditor

Formal and factual tone

Complete decision context

Suitable for audit logs

All explanations originate from the same decision payload.

GenAI Design Constraints

LLM access via OpenRouter

Models: Mistral / LLaMA (7Bâ€“8B class)

Low temperature, capped token generation

No fine-tuning

No RAG

No LangChain or agent frameworks

GenAI is used strictly as a controlled explanation generator.

Tech Stack

Backend API: FastAPI

ML Model: XGBoost (external)

Explainability: SHAP (external)

GenAI: OpenRouter

UI: Lovable (API-driven, no frontend logic)

Demo

The demo uses fixed, reproducible decision payloads located in:

demo/sample_inputs/


The same payload can be explained from multiple perspectives by switching
roles, demonstrating explainability without decision variance.

Design Principles

Decision logic and explanation logic are isolated

Interfaces are frozen early

Compliance takes precedence over creativity

Minimal infrastructure, maximum clarity

License

MIT License



